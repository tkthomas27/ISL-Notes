---
title: 'Chapter 6: Linear Model Selection and Regularization'
pdf_document: default
output:
  html_notebook:
    pandoc_args:
    - --number-sections
    - --number-offset=6
    toc: yes
  html_document:
    toc: yes
  pdf_document:
    toc: yes
---

# Overview

Problem: high dimensionality

## Subset Selection
variations
pros, cons

## Shrinkage Methods

## Dimension Reduction

# Lab

## Best Subset Selection

### Original

```{r, message=FALSE}
library(ISLR)

Hitters <- na.omit(Hitters) # remove na values
sum(is.na(Hitters)) # check that na values are removed

# use leaps to find best subset selection by r2
library(leaps)
regfit.full <- regsubsets(Salary ~., Hitters, nvmax = 10)
summary(regfit.full)
```

```{r}
reg.summary <- summary(regfit.full) #summarize models
reg.summary$rsq #see the r2s
```

Plot the the change in RSS and Rsq as the number of variables increase

```{r, message=FALSE}
par(mfrow=c(1,2))
plot(reg.summary$rss, xlab = "Nvar", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Nvar", ylab = "Adj RSq", type = "l")
best_model <- which.max(reg.summary$adjr2)
points(10, reg.summary$adjr2[best_model], col="red", cex = 2, pch=20)
```

Look at the variables selecgted for the best model under different criteria.

```{r}
par(mfrow=c(1,2))
plot(regfit.full, scale = "r2")
plot(regfit.full, scale = "adjr2")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")
```

Look at the coefficients for the sixth model

```{r}
coef(regfit.full ,6)
```


```{r, message=FALSE}
#forward subsetselection
regfit.fwd <- regsubsets(Salary~., data = Hitters, nvmax = 19, method = "forward")
#summary(regfit.fwd)

#backward subsetselection
regfit.bwd <- regsubsets(Salary~., data = Hitters, nvmax = 19, method = "backward")
#summary(regfit.bwd)
```

### Better

https://gist.github.com/dkahle/7942a7eba8aaa026d0bab6a1e9d88580

https://cran.r-project.org/web/packages/dotwhisker/vignettes/dotwhisker-vignette.html


```{r}
library(dotwhisker)

x <- broom::tidy(regfit.full)

dwplot(regfit.full)

m1 <- lm(mpg ~ wt + cyl + disp + gear, data = mtcars)
dwplot(m1)
```


## Choosing the Best Model

### Original

Regular test/train split;

but regsubsets does not have prediction function; some manual work 

```{r}
# split into train/test set
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(Hitters), rep = TRUE)
test <- (!train)

# fit subset selection to test data
regfit.best = regsubsets(Salary ~ ., data = Hitters[train,], nvmax = 19)

# ?
test.mat = model.matrix(Salary ~ ., data = Hitters[test,]) 

# loop 
val.errors <- rep(NA, 19)
for(i in 1:19){
    coefi <- coef(regfit.best, id = i)
    pred <- test.mat[,names(coefi)]%*%coefi
    val.errors[i] <- mean((Hitters$Salary[test] - pred)^2)
}

# find the best model
best_model <- which.min(val.errors)

# predict for regsubsets function
predict.regsubsets = function(object, newdata, id, ...){
    form = as.formula(object$call[[2]])
    mat = model.matrix(form, newdata)
    coefi = coef(object, id = id)
    xvars = names(coef)
    mat[,xvars]%*%coefi
}

```

best model using cross validation
Note: original uses predict and not predict.regsubsets; it has been corrected


```{r}
k = 10
set.seed(1)

folds = sample(1:k, nrow(Hitters), replace = TRUE)
cv.errors = matrix(NA, k, 19, dimnames = list(NULL, paste(1:19)))

for(j in 1:k){
    best.fit = regsubsets(Salary ~ ., data = Hitters[folds!=j,], nvmax = 19)
    for(i in 1:19){
        pred = predict(best.fit, Hitters[folds == j,], id = i)
        cv.errors[j,i] = mean((Hitters$Salary[folds == j] - pred)^2)
    }
}

mean.cv.errors = apply(cv.errors, 2, mean)
```

```{r}
par(mfrow = c(1,1))
plot(mean.cv.errors, type = 'b')
```

