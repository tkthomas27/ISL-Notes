---
title: 'Chapter 8: Tree-Based Methods'
pdf_document: default
output:
  html_notebook:
    pandoc_args:
    - --number-sections
    - --number-offset=8
    toc: yes
  html_document:
    toc: yes
  pdf_document:
    toc: yes
---

\setcounter{section}{8}

## Basics of Decision Trees

* Stratifying/segmenting predictor space into simple regions
* Useful and easy to interpret but not as powerful
* bagging, random forests, and boosting help
* Can be used for regression and classification

### Regression Trees


\subsubsection{Classification Trees}

\subsubsection{Tree Versus Linear Models}

\subsubsection{Advantages and Disadvantages of Trees}


\subsection{Bagging, Random Forests, Boosting}

\subsection{Bagging}

Bootstrap aggregating (bagging) is a general purpose tool for reducing variance of statistical methods; frequently used with decision trees

\subsection{Random Forests}

decorrelates the tree

a change to look at git hub app

\subsection{Boosting}

\section{Lab}

```{r}
library(tree)
library(ISLR)

#-------
#classification tree
#-------
High <- ifelse(Carseats$Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)

# fit tree
tree.carseats <- tree(High ~ .-Sales, Carseats)
summary(tree.carseats)

plot(tree.carseats)
text(tree.carseats, pretty = 0)

#test prediction of tree
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train,]
High.test <- High[-train]
tree.carseats <- tree(High~.-Sales, Carseats, subset = train)
tree.pred <- predict(tree.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)

#cross validate and prune
set.seed(3)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)

#we see that 9 is the best
par(mfrow = c(1,2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")

prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats)

# test pruned tree
tree.pred <- predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
```

Regression Tree

```{r}
library(MASS)
set.seed(1)

#-------
#regression tree
#-------

train <- sample(1:nrow(Boston), nrow(Boston)/2)
tree.Boston <- tree(medv~., Boston, subset = train)
summary(tree.Boston)

#plot
plot(tree.Boston)
text(tree.Boston, pretty = 0)

#cross validate and prune
cv.boston <- cv.tree(tree.Boston)
plot(cv.boston$size, cv.boston$dev, type = 'b')

#cv shows most complex tree is best; but can prune anyway
prune.boston <- prune.tree(tree.Boston, best = 5)
plot(prune.boston)
text(prune.boston, pretty=0)

#test unpruned tree
#newdata?
yhat <- predict(tree.Boston, newdata = Boston[-train,])
boston.test <- Boston[-train, "medv"]
plot(yhat, boston.test)
abline(0,1)
mean((yhat - boston.test)^2)
```

```{r}
#bagging and randomforests
library(randomForest)
set.seed(1)

bag.boston <- randomForest(medv~., data = Boston, subset = train, mtry = 13, importance = TRUE)
bag.boston

yhat.bag <- predict(bag.boston, newdata = Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag - boston.test)^2)

#can change ntree and mtry (lower mtry similar to pruning)

importance(bag.boston)

varImpPlot(bag.boston)
```

```{r}
#boosting
library(gbm)
set.seed(1)

boost.boston <- gbm(medv~., data = Boston[train,], distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
summary(boost.boston)

#partial dependence plots
par(mfrow=c(1,2))
plot(boost.boston, i='rm')
plot(boost.boston, i='lstat')

yhat.boost <- predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)
mean((yhat.boost - boston.test)^2)

# change tuning parameter
boost.boston <- gbm(medv~., data = Boston[train,], distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F)
summary(boost.boston)
yhat.boost <- predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
```

















